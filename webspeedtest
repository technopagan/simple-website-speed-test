#!/usr/bin/env bash 

###############################################################################
#
# Bash script to initiate TAP compliant (http://www.testanything.org/) web 
# performance metrics testing to be easily integrated with Jenkins or Travis CI
#
# Usage: ./webspeedtest microsite.bats http://your-url.tld/somepage
#
###############################################################################
#
# Tools needed to run this test suite:
#
#	* Node.js (http://github.com/joyent/node)
#
# * PhantomJS (http://github.com/ariya/phantomjs/)
#
#	* Phantomas (http://github.com/macbre/phantomas)
#
#	* jq (http://stedolan.github.io/jq/)
#
#	* BATS (http://github.com/sstephenson/bats)
#
#
#
# For potential integration with Jenkins, check out this plugin:
#
# * https://wiki.jenkins-ci.org/display/JENKINS/TAP+Plugin
#
###############################################################################
# 
# How to maintain the test suite:
#
# All names for tests & their variables are derived from JSON object names
# used by Phantomas (https://github.com/macbre/phantomas) for consistency.
#
# To create a new testable metric, find its respective JSON object name in
# the Phantomas documentation (https://github.com/macbre/phantomas#metrics).
#
# Then add it to the array of test metric names within "Metrics_to_analyze"
# in this script and create a test, using its name as the variable containing
# the testable value, for it in "performance-tests.bats".
#
# Example - testing that the time to first byte is less than 200ms:
#
# - performance-tests.sh:
#		[...]
#		Metrics_to_analyze=('timeToFirstByte' '...' '...' '...')
#		[...]
# 
# - performance-tests.bats:
#		[...]
# 	@test "Time to first byte" {
#  		[ "$timeToFirstByte" -lt 180 ]
# 	}
#		[...]
# 
###############################################################################



###############################################################################
# USER CONFIGURABLE PARAMETERS
###############################################################################

# Define an array of metrics to be analyzed  // 
Metrics_to_analyze=('timeToFirstByte' 'medianResponse' 'requests' 'domains' 'notFound' 'redirects' 'cssCount' 'cssSize' 'jsCount' 'jsSize' 'webfontCount' 'DOMelementMaxDepth')

# How often should we run the test to arrive at a reliable median
Iterations=5



###############################################################################
# GLOBAL RUNTIME VARIABLES (usually do not require tuning by user)
###############################################################################

# Define against which test suite retrieved metrics will run
Test_suite="$1"

# Accept the test URL as a command  line argument 
URL_to_measure="$2"



###############################################################################
# MAIN PROGRAM
###############################################################################

main() {
	# Run the performance metrics collector multiple times so that we can later calculate stable results such as Median etc.
	for((i=0;i<${Iterations};i++)) ; do
		retrieve_phantomas_performance_metrics Phantomas_JSON_output "${URL_to_measure}"
		Phantomas_JSON_output_array[${i}]=${Phantomas_JSON_output}
		# echo ${Phantomas_JSON_output_array[i]}
	done
	# For each metric we want to measure, retrieve its value from each instance of our gathered performance metrics to create arrays of retrieved data per metric 
	for((m=0;m<${#Metrics_to_analyze[@]};m++)) ; do
		for((p=0;p<${#Phantomas_JSON_output_array[@]};p++)) ; do
			# For each instance of the Phantomas JSON data, retrieve the desired metric
			slice_metric Single_Metric "${Metrics_to_analyze[m]}" "${p}"
			# Store all retrieved values of the desired metric in an new array
			Sliced_metric_array[${p}]=${Single_Metric}
		done
		# For each array of values for a specific metric, calculate the Median (Mean)
		calculate_mean Mean_metric Sliced_metric_array[@]
		# Hand off the Mean result to a variable of identical name to the Phantomas metric and export it for access by BATS
		eval ${Metrics_to_analyze[$m]}="${Mean_metric}"
		export ${Metrics_to_analyze[$m]}
	done
	# After all iterations and calculations are done, start the matrics test suite
	bats --tap "${Test_suite}"
}



###############################################################################
# FUNCTIONS
###############################################################################

# For a user-specified URL, retrieve performance metrics via Phantomas 
function retrieve_phantomas_performance_metrics {
	# Define local variables to work with
	local  __result=$1
  	local  __test_url=$2
	# Use Phantomas to gather performance metrics in JSON format
	local  __phantomas_json_output=$(phantomas --format=json --url "${__test_url}")
	# Return the result
	eval $__result="'${__phantomas_json_output}'"
}

# Using a CLI JSON parser, such as jq, extract single metrics from the main JSON response body
function slice_metric {
	# Define local variables to work with
	local  __result=$1
  	local  __metric=$2
  	local  __test_instance=$3
	# With jq, let's slice through our response JSON and retrieve the desired metric
	local  __sliced_metric=$(echo "${Phantomas_JSON_output_array[__test_instance]}" | jq ".metrics."${__metric})
	# Return the result
	eval $__result="'${__sliced_metric}'"
}

# For arrays of values for a single metric, calculate the Mean (Median) value to prevent random outliers from making tests fail
function calculate_mean {
	# Define local variables to work with
	local  __result=$1
  	# BASH needs a rather complex syntax to accept an array as input
  	declare -a __array_to_process=("${!2}")
  	# Sort the array to ascending in numerical value
	local  __array_to_process_sorted=($(printf '%s\n' "${__array_to_process[@]}" | sort -n))
	# Find the Mean in the middle of the array index
	local __metric_median=${__array_to_process_sorted[$(echo $((${#__array_to_process_sorted[@]}/2)))]}
	# Return the result
	eval $__result="'${__metric_median}'"
}

# Lastly, initiate the main function to start the entire process 
main
